{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建SparkSession对象\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('random_forest').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "path = r\"G:\\LKM\\PySark机器学习、自然语言处理与推荐系统\\随机森林\\affairs.csv\"\n",
    "df = spark.read.csv(path, inferSchema=True, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(6366, 6)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查数据集的形状结构\n",
    "(df.count(), len(df.columns))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rate_marriage: integer (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- yrs_married: double (nullable = true)\n",
      " |-- children: double (nullable = true)\n",
      " |-- religious: integer (nullable = true)\n",
      " |-- affairs: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 验证输入值的数据类型\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+-----------+--------+---------+-------+\n",
      "|rate_marriage| age|yrs_married|children|religious|affairs|\n",
      "+-------------+----+-----------+--------+---------+-------+\n",
      "|            5|32.0|        6.0|     1.0|        3|      0|\n",
      "|            4|22.0|        2.5|     0.0|        2|      0|\n",
      "|            3|32.0|        9.0|     3.0|        3|      1|\n",
      "|            3|27.0|       13.0|     3.0|        1|      1|\n",
      "|            4|22.0|        2.5|     0.0|        1|      1|\n",
      "+-------------+----+-----------+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看前五条数据\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|     rate_marriage|               age|      yrs_married|          children|         religious|\n",
      "+-------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|              6366|              6366|             6366|              6366|              6366|\n",
      "|   mean| 4.109644989004084|29.082862079798932| 9.00942507068803|1.3968740182218033|2.4261702796104303|\n",
      "| stddev|0.9614295945655025| 6.847881883668817|7.280119972766412| 1.433470828560344|0.8783688402641785|\n",
      "|    min|                 1|              17.5|              0.5|               0.0|                 1|\n",
      "|    max|                 5|              42.0|             23.0|               5.5|                 4|\n",
      "+-------+------------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用describe函数来检查数据集的统计指标\n",
    "df.describe().select('summary', 'rate_marriage', 'age', 'yrs_married', 'children', 'religious').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1| 2053|\n",
      "|      0| 4313|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看类别出现次数\n",
    "df.groupBy('affairs').count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|rate_marriage|count|\n",
      "+-------------+-----+\n",
      "|            1|   99|\n",
      "|            3|  993|\n",
      "|            5| 2684|\n",
      "|            4| 2242|\n",
      "|            2|  348|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看婚姻评分人数情况\n",
    "df.groupBy('rate_marriage').count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+\n",
      "|rate_marriage|affairs|count|\n",
      "+-------------+-------+-----+\n",
      "|            1|      0|   25|\n",
      "|            1|      1|   74|\n",
      "|            2|      0|  127|\n",
      "|            2|      1|  221|\n",
      "|            3|      0|  446|\n",
      "|            3|      1|  547|\n",
      "|            4|      0| 1518|\n",
      "|            4|      1|  724|\n",
      "|            5|      0| 2197|\n",
      "|            5|      1|  487|\n",
      "+-------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看婚姻评分与婚外恋的关系\n",
    "df.groupBy('rate_marriage', 'affairs').count().orderBy('rate_marriage', 'affairs', 'count', ascending=True).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+\n",
      "|religious|affairs|count|\n",
      "+---------+-------+-----+\n",
      "|        1|      0|  613|\n",
      "|        1|      1|  408|\n",
      "|        2|      0| 1448|\n",
      "|        2|      1|  819|\n",
      "|        3|      0| 1715|\n",
      "|        3|      1|  707|\n",
      "|        4|      0|  537|\n",
      "|        4|      1|  119|\n",
      "+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看宗教与婚外恋的关系\n",
    "df.groupBy('religious', 'affairs').count().orderBy('religious', 'affairs', 'count', ascending=True).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+\n",
      "|children|affairs|count|\n",
      "+--------+-------+-----+\n",
      "|     0.0|      0| 1912|\n",
      "|     0.0|      1|  502|\n",
      "|     1.0|      0|  747|\n",
      "|     1.0|      1|  412|\n",
      "|     2.0|      0|  873|\n",
      "|     2.0|      1|  608|\n",
      "|     3.0|      0|  460|\n",
      "|     3.0|      1|  321|\n",
      "|     4.0|      0|  197|\n",
      "|     4.0|      1|  131|\n",
      "|     5.5|      0|  124|\n",
      "|     5.5|      1|   79|\n",
      "+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 子女数量与婚外恋的关系\n",
    "df.groupBy('children', 'affairs').count().orderBy('children', 'affairs', 'count', ascending=True).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "|affairs|avg(rate_marriage)|          avg(age)|  avg(yrs_married)|     avg(children)|    avg(religious)|avg(affairs)|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "|      1|3.6473453482708234|30.537018996590355|11.152459814905017|1.7289332683877252| 2.261568436434486|         1.0|\n",
      "|      0| 4.329700904242986| 28.39067934152562| 7.989334569904939|1.2388128912589844|2.5045212149316023|         0.0|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 婚外恋的情况\n",
    "df.groupBy('affairs').mean().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rate_marriage: integer (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- yrs_married: double (nullable = true)\n",
      " |-- children: double (nullable = true)\n",
      " |-- religious: integer (nullable = true)\n",
      " |-- affairs: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用Spark的VectorAssembler来创建合并所有输入特征的单个向量\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# 将所有的列组装成单个向量，该向量会充当模型的输入特征\n",
    "df_assembler = VectorAssembler(inputCols=['rate_marriage', 'age', 'yrs_married', 'children', 'religious'], outputCol='features')\n",
    "df = df_assembler.transform(df)\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------+\n",
      "|features               |affairs|\n",
      "+-----------------------+-------+\n",
      "|[5.0,32.0,6.0,1.0,3.0] |0      |\n",
      "|[4.0,22.0,2.5,0.0,2.0] |0      |\n",
      "|[3.0,32.0,9.0,3.0,3.0] |1      |\n",
      "|[3.0,27.0,13.0,3.0,1.0]|1      |\n",
      "|[4.0,22.0,2.5,0.0,1.0] |1      |\n",
      "|[4.0,37.0,16.5,4.0,3.0]|1      |\n",
      "|[5.0,27.0,9.0,1.0,1.0] |1      |\n",
      "|[4.0,27.0,9.0,0.0,2.0] |1      |\n",
      "|[5.0,37.0,23.0,5.5,2.0]|1      |\n",
      "|[5.0,37.0,23.0,5.5,2.0]|1      |\n",
      "+-----------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看特征工程后的数据\n",
    "df.select(['features', 'affairs']).show(10, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# 截取训练模型的数据集\n",
    "model_df = df.select(['features', 'affairs'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1| 1573|\n",
      "|      0| 3268|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集\n",
    "train_df, test_df = model_df.randomSplit([0.75, 0.25])\n",
    "\n",
    "# 查看训练集的分布\n",
    "train_df.groupBy('affairs').count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1|  480|\n",
      "|      0| 1045|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看训练集的分布\n",
    "test_df.groupBy('affairs').count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 构建和训练随机森林模型\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(labelCol='affairs', numTrees=50).fit(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|            features|affairs|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.0,27.0,2.5,0.0...|      1|[21.5524713378311...|[0.43104942675662...|       1.0|\n",
      "|[1.0,27.0,6.0,0.0...|      0|[18.5396362370975...|[0.37079272474195...|       1.0|\n",
      "|[1.0,27.0,6.0,1.0...|      1|[19.2769148479397...|[0.38553829695879...|       1.0|\n",
      "|[1.0,27.0,6.0,2.0...|      1|[18.4123409983753...|[0.36824681996750...|       1.0|\n",
      "|[1.0,27.0,9.0,2.0...|      1|[15.9422892279239...|[0.31884578455847...|       1.0|\n",
      "|[1.0,27.0,13.0,2....|      1|[15.8192193765475...|[0.31638438753095...|       1.0|\n",
      "|[1.0,32.0,9.0,3.0...|      1|[15.9567556143388...|[0.31913511228677...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[15.8336857629624...|[0.31667371525924...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[16.1003696511144...|[0.32200739302228...|       1.0|\n",
      "|[1.0,32.0,16.5,3....|      1|[15.1131270353138...|[0.30226254070627...|       1.0|\n",
      "|[1.0,32.0,16.5,3....|      1|[16.1724744409459...|[0.32344948881891...|       1.0|\n",
      "|[1.0,37.0,16.5,1....|      1|[16.0949027874513...|[0.32189805574902...|       1.0|\n",
      "|[1.0,42.0,16.5,1....|      1|[14.6925671528799...|[0.29385134305759...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[15.3123643996487...|[0.30624728799297...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[15.3123643996487...|[0.30624728799297...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[15.3123643996487...|[0.30624728799297...|       1.0|\n",
      "|[1.0,42.0,23.0,3....|      1|[15.4929939396002...|[0.30985987879200...|       1.0|\n",
      "|[1.0,42.0,23.0,3....|      1|[16.5950027933733...|[0.33190005586746...|       1.0|\n",
      "|[1.0,42.0,23.0,4....|      0|[18.7393527649278...|[0.37478705529855...|       1.0|\n",
      "|[1.0,42.0,23.0,4....|      1|[18.7393527649278...|[0.37478705529855...|       1.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 基于测试数据进项评估\n",
    "model_predict = model.transform(test_df)\n",
    "model_predict.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0| 1248|\n",
      "|       1.0|  277|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看预测结果分布\n",
    "model_predict.groupBy('prediction').count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# 导入评估模块\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7239344262295082"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算准确率\n",
    "accuracy = MulticlassClassificationEvaluator(labelCol='affairs', metricName='accuracy').evaluate(model_predict)\n",
    "accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7048322187370539"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算精度\n",
    "precision = MulticlassClassificationEvaluator(labelCol='affairs', metricName='weightedPrecision').evaluate(model_predict)\n",
    "precision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7294946172248804"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算AUC曲线下的面积\n",
    "auc = BinaryClassificationEvaluator(labelCol='affairs').evaluate(model_predict)\n",
    "auc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "SparseVector(5, {0: 0.5912, 1: 0.0224, 2: 0.2394, 3: 0.063, 4: 0.0841})"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算特征的重要性\n",
    "model.featureImportances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "{'numeric': [{'idx': 0, 'name': 'rate_marriage'},\n  {'idx': 1, 'name': 'age'},\n  {'idx': 2, 'name': 'yrs_married'},\n  {'idx': 3, 'name': 'children'},\n  {'idx': 4, 'name': 'religious'}]}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将特征映射数值\n",
    "df.schema['features'].metadata['ml_attr']['attrs']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2927.save.\n: java.io.IOException: Path G:\\LKM\\PySark机器学习、自然语言处理与推荐系统\\随机森林 already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\r\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-29-bc3e3ff0653e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mml\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclassification\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRandomForestClassificationModel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mr\"G:\\LKM\\PySark机器学习、自然语言处理与推荐系统\\随机森林\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\anaconda3\\envs\\lkm\\lib\\site-packages\\pyspark\\ml\\util.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(self, path)\u001B[0m\n\u001B[0;32m    222\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m         \u001B[1;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 224\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    225\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\envs\\lkm\\lib\\site-packages\\pyspark\\ml\\util.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(self, path)\u001B[0m\n\u001B[0;32m    173\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbasestring\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    174\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"path should be a basestring, got type %s\"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 175\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    176\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    177\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0moverwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\envs\\lkm\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[1;32m-> 1305\u001B[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[0;32m   1306\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1307\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\envs\\lkm\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    129\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    130\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 131\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    132\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    133\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\envs\\lkm\\lib\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 328\u001B[1;33m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[0;32m    329\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o2927.save.\n: java.io.IOException: Path G:\\LKM\\PySark机器学习、自然语言处理与推荐系统\\随机森林 already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\r\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "model.save(r\"G:\\LKM\\PySark机器学习、自然语言处理与推荐系统\\随机森林\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "lkm",
   "language": "python",
   "display_name": "Python [conda env:lkm]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}